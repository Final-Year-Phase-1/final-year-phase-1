{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T08:42:19.137609Z",
     "iopub.status.busy": "2025-09-17T08:42:19.137405Z",
     "iopub.status.idle": "2025-09-17T10:23:06.798100Z",
     "shell.execute_reply": "2025-09-17T10:23:06.797491Z",
     "shell.execute_reply.started": "2025-09-17T08:42:19.137591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Updated Kaggle cell: deterministic injections + GAF visuals + improved confusion matrices + t-SNE\n",
    "# Edit DATA_FOLDER to point to your unzipped SingleHopLabelledReadings folder in Kaggle.\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# ---------------- Configuration (edit these if you want faster runs) ----------------\n",
    "DATA_FOLDER = \"/kaggle/input/singlehoplabelledreadings/SingleHopLabelledReadings\"  # <- edit if needed\n",
    "OUTPUT_DIR = \"/kaggle/working/fewshot_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Defaults (paper-like). Reduce for quick tests.\n",
    "SAMPLES_PER_CLASS = 550    # more samples -> better classification stability\n",
    "SERIES_LEN = 100\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 100               # finaetune epochs on support set (paper used 100)\n",
    "N_EPISODES = 200           # number of episodes to average (paper averages many)\n",
    "QUERY_M = 15\n",
    "K_VALUES = [1, 5, 10]\n",
    "BACKBONES = [\"resnet18\", \"vgg16\", \"mobilenet_v2\"]\n",
    "FINETUNE_MODES = [\"no_finetune\", \"finetune_last\", \"finetune_whole\"]\n",
    "\n",
    "TSNE_N_ITER = 1000\n",
    "\n",
    "# ---------------- Helpers: load dataset & fix typos ----------------\n",
    "def load_singlehop_temperature(data_folder: str) -> np.ndarray:\n",
    "    files = sorted(Path(data_folder).glob(\"*.txt\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .txt files found in {data_folder}. Check dataset path.\")\n",
    "    all_temps = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=r\"\\s+\", engine=\"python\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file}, skipping. Error: {e}\")\n",
    "            continue\n",
    "        df = df.dropna(how=\"all\")\n",
    "        temp_col = None\n",
    "        for col in df.columns:\n",
    "            c = str(col).lower()\n",
    "            if \"temp\" in c or \"tep\" in c or \"tempr\" in c:\n",
    "                temp_col = col\n",
    "                break\n",
    "        if temp_col is None:\n",
    "            temp_col = df.columns[-1]\n",
    "            print(f\"[WARN] No temp-like col in {file.name}, using last col: {temp_col}\")\n",
    "        # keep only numeric rows\n",
    "        df = df[pd.to_numeric(df[temp_col], errors=\"coerce\").notnull()]\n",
    "        temps = df[temp_col].astype(np.float32).values\n",
    "        all_temps.append(temps)\n",
    "        print(f\"Loaded {len(temps)} rows from {file.name} (column: {temp_col})\")\n",
    "    combined = np.concatenate(all_temps)\n",
    "    print(f\"Combined temperature series length: {len(combined)}\")\n",
    "    return combined\n",
    "\n",
    "# ---------------- Deterministic Fault injections ----------------\n",
    "# We use fixed parameters (no randomness) to make visuals repeatable and closer to paper examples.\n",
    "\n",
    "def inject_drift_det(x, drift_rate=0.05, start_idx=20):\n",
    "    \"\"\"Deterministic linear drift starting at fixed index.\"\"\"\n",
    "    y = x.copy()\n",
    "    for i in range(start_idx, len(x)):\n",
    "        y[i] += (i - start_idx + 1) * drift_rate\n",
    "    return y\n",
    "\n",
    "def inject_stuck_det(x, stuck_value_offset=5.0, start_idx=30, length=25):\n",
    "    \"\"\"Deterministic stuck: a block set to a constant (value + offset).\"\"\"\n",
    "    stuck_value = np.mean(x) + stuck_value_offset\n",
    "    y = x.copy()\n",
    "    end = min(len(x), start_idx + length)\n",
    "    y[start_idx:end] = stuck_value\n",
    "    return y\n",
    "\n",
    "def inject_bias_det(x, bias_val=3.0, instances=(15, 45, 75), instance_len=8):\n",
    "    \"\"\"Deterministic bias occurrences at fixed positions.\"\"\"\n",
    "    y = x.copy()\n",
    "    for start in instances:\n",
    "        end = min(len(x), start + instance_len)\n",
    "        y[start:end] += bias_val\n",
    "    return y\n",
    "\n",
    "def inject_spike_det(x, amp=4.0, freq=6):\n",
    "    \"\"\"Deterministic spikes at fixed frequency (every freq samples).\"\"\"\n",
    "    y = x.copy()\n",
    "    for idx in range(freq, len(x), freq):\n",
    "        if idx < len(x):\n",
    "            y[idx: min(len(x), idx+2)] += amp\n",
    "    return y\n",
    "\n",
    "def inject_erratic_det(x, var_scale=0.6):\n",
    "    \"\"\"Deterministic erratic noise using seeded numpy RNG.\"\"\"\n",
    "    rng = np.random.RandomState(SEED)  # fixed seed for determinism\n",
    "    noise = rng.normal(0, var_scale, size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def inject_dataloss_det(x, block_positions=(5,40,70), block_len=6):\n",
    "    \"\"\"Deterministic data-loss: blocks replaced with near-zero at fixed positions.\"\"\"\n",
    "    y = x.copy()\n",
    "    for start in block_positions:\n",
    "        end = min(len(x), start + block_len)\n",
    "        y[start:end] = 1e-6\n",
    "    return y\n",
    "\n",
    "# For dataset generation we use deterministic versions above.\n",
    "inject_drift = inject_drift_det\n",
    "inject_stuck = inject_stuck_det\n",
    "inject_bias_instances = inject_bias_det\n",
    "inject_spike = inject_spike_det\n",
    "inject_erratic = inject_erratic_det\n",
    "inject_data_loss = inject_dataloss_det\n",
    "\n",
    "# ---------------- GAF conversion (GASF) ----------------\n",
    "def time_series_to_gaf(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    # Per-sample rescaling to [-1,1] robustly\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    if xmax == xmin:\n",
    "        x_scaled = np.zeros_like(x)\n",
    "    else:\n",
    "        x_scaled = 2 * (x - xmin) / (xmax - xmin) - 1.0\n",
    "        x_scaled = np.clip(x_scaled, -1 + 1e-8, 1 - 1e-8)\n",
    "    phi = np.arccos(x_scaled)\n",
    "    # Gramian Angular Summation Field (GASF): cos(phi_i + phi_j)\n",
    "    gaf = np.cos(np.add.outer(phi, phi))\n",
    "    # enforce numeric range [-1,1]\n",
    "    gaf = np.clip(gaf, -1.0, 1.0)\n",
    "    return gaf.astype(np.float32)\n",
    "\n",
    "# ---------------- Build synthetic dataset (deterministic fault parameters) ----------------\n",
    "def build_synthetic_dataset(series, samples_per_class=SAMPLES_PER_CLASS, series_len=SERIES_LEN):\n",
    "    classes = {\n",
    "        0: 'normal', 1: 'drift', 2: 'stuck',\n",
    "        3: 'bias', 4: 'spike', 5: 'erratic', 6: 'data_loss'\n",
    "    }\n",
    "    images = []\n",
    "    labels = []\n",
    "    L = len(series)\n",
    "    # To make dataset reproducible, use a deterministic set of start indices (uniformly spaced)\n",
    "    max_start = max(0, L - series_len - 1)\n",
    "    starts = np.linspace(0, max_start, num=samples_per_class, dtype=int) if max_start>0 else np.zeros(samples_per_class, dtype=int)\n",
    "    for cls in classes:\n",
    "        for sidx in starts:\n",
    "            x = series[sidx:sidx+series_len].copy()\n",
    "            # small deterministic jitter using seeded RNG to avoid identical windows\n",
    "            jitter_rng = np.random.RandomState(sidx + cls + SEED)\n",
    "            x += jitter_rng.normal(0, 0.01*np.std(x)+1e-6, size=x.shape)\n",
    "            if cls == 0:\n",
    "                y = x\n",
    "            elif cls == 1:\n",
    "                y = inject_drift(x, drift_rate=0.05, start_idx=20)\n",
    "            elif cls == 2:\n",
    "                y = inject_stuck(x, stuck_value_offset=5.0, start_idx=30, length=25)\n",
    "            elif cls == 3:\n",
    "                y = inject_bias_instances(x, bias_val=3.0, instances=(15,45,75), instance_len=8)\n",
    "            elif cls == 4:\n",
    "                y = inject_spike(x, amp=4.0, freq=6)\n",
    "            elif cls == 5:\n",
    "                y = inject_erratic(x, var_scale=0.6)\n",
    "            elif cls == 6:\n",
    "                y = inject_data_loss(x, block_positions=(5,40,70), block_len=6)\n",
    "            gaf = time_series_to_gaf(y)\n",
    "            images.append(gaf)\n",
    "            labels.append(cls)\n",
    "    images = np.stack(images)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    print(\"Built synthetic dataset:\", images.shape, labels.shape)\n",
    "    return images, labels\n",
    "\n",
    "# ---------------- Dataset sampler ----------------\n",
    "class SyntheticGAFDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.classes = sorted(np.unique(labels))\n",
    "        self.cls_indices = {c: np.where(labels == c)[0].tolist() for c in self.classes}\n",
    "\n",
    "    def sample_episode(self, n_way=7, k_shot=1, n_query=QUERY_M):\n",
    "        classes = random.sample(self.classes, n_way)\n",
    "        support_idx, query_idx = [], []\n",
    "        support_lbls, query_lbls = [], []\n",
    "        for i, c in enumerate(classes):\n",
    "            inds = random.sample(self.cls_indices[c], k_shot + n_query)\n",
    "            s = inds[:k_shot]; q = inds[k_shot:]\n",
    "            support_idx += s; query_idx += q\n",
    "            support_lbls += [i] * k_shot\n",
    "            query_lbls += [i] * n_query\n",
    "        sup_imgs = self.images[support_idx]\n",
    "        qry_imgs = self.images[query_idx]\n",
    "        return sup_imgs, np.array(support_lbls), qry_imgs, np.array(query_lbls), classes\n",
    "\n",
    "# ---------------- Backbones & embedding extraction ----------------\n",
    "def get_backbone(name=\"resnet18\", pretrained=True):\n",
    "    # use weights keyword to avoid deprecation warning\n",
    "    if name == \"resnet18\":\n",
    "        try:\n",
    "            from torchvision.models import ResNet18_Weights\n",
    "            model = models.resnet18(weights=ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        except Exception:\n",
    "            model = models.resnet18(pretrained=pretrained)\n",
    "        feat_dim = model.fc.in_features\n",
    "        backbone = nn.Sequential(*list(model.children())[:-1])\n",
    "    elif name == \"vgg16\":\n",
    "        try:\n",
    "            from torchvision.models import VGG16_Weights\n",
    "            model = models.vgg16(weights=VGG16_Weights.DEFAULT if pretrained else None)\n",
    "        except Exception:\n",
    "            model = models.vgg16(pretrained=pretrained)\n",
    "        feat_dim = 512\n",
    "        backbone = nn.Sequential(*list(model.features))\n",
    "    elif name == \"mobilenet_v2\":\n",
    "        try:\n",
    "            from torchvision.models import MobileNet_V2_Weights\n",
    "            model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT if pretrained else None)\n",
    "        except Exception:\n",
    "            model = models.mobilenet_v2(pretrained=pretrained)\n",
    "        feat_dim = 1280\n",
    "        backbone = nn.Sequential(*list(model.features))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown backbone\")\n",
    "    return backbone.to(DEVICE), feat_dim\n",
    "\n",
    "def extract_embeddings(backbone, x):\n",
    "    feats = backbone(x)\n",
    "    if feats.ndim == 4:\n",
    "        feats = F.adaptive_avg_pool2d(feats, (1,1)).reshape(feats.size(0), -1)\n",
    "    else:\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "    return feats\n",
    "\n",
    "# ---------------- Prototypical helpers ----------------\n",
    "def compute_prototypes(emb_sup, sup_labels, n_way=7):\n",
    "    prototypes = []\n",
    "    for c in range(n_way):\n",
    "        mask = (sup_labels == c)\n",
    "        proto = emb_sup[mask].mean(dim=0)\n",
    "        prototypes.append(proto)\n",
    "    return torch.stack(prototypes, dim=0)\n",
    "\n",
    "def prototypical_predict(emb_qry, prototypes):\n",
    "    dists = torch.cdist(emb_qry, prototypes)\n",
    "    logits = -dists\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# ---------------- Finetune head (finetune_last) - stronger LR ----------------\n",
    "def finetune_head(backbone, feat_dim, sup_tensor, sup_labels, n_way=7, epochs=EPOCHS, lr=1e-3):\n",
    "    backbone.eval()\n",
    "    for p in backbone.parameters(): p.requires_grad = False\n",
    "    head = nn.Linear(feat_dim, n_way).to(DEVICE)\n",
    "    opt = torch.optim.Adam(head.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    sup_labels_t = sup_labels.to(DEVICE)\n",
    "    for _ in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            feats = extract_embeddings(backbone, sup_tensor)\n",
    "        logits = head(feats)\n",
    "        loss = loss_fn(logits, sup_labels_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return head\n",
    "\n",
    "# ---------------- Transforms ----------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ---------------- GAF display helper (jet, vmin/vmax fixed) ----------------\n",
    "def show_gaf_grid(images, labels, class_names, examples_per_class=3, savepath=None):\n",
    "    classes = sorted(np.unique(labels))\n",
    "    fig_h = len(classes) * 1.8\n",
    "    fig_w = examples_per_class * 1.8\n",
    "    fig, axes = plt.subplots(len(classes), examples_per_class, figsize=(fig_w, fig_h))\n",
    "    if len(classes) == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    for i, c in enumerate(classes):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        chosen = np.random.choice(idxs, size=examples_per_class, replace=False)\n",
    "        for j, idx in enumerate(chosen):\n",
    "            im = images[idx]\n",
    "            ax = axes[i, j]\n",
    "            # im values in [-1,1], fix vmin/vmax for consistent coloring\n",
    "            ax.imshow(im, cmap=\"jet\", aspect='auto', vmin=-1.0, vmax=1.0)\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f\"{class_names[c]}\", rotation=0, labelpad=40, va='center')\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Fault example line plots (deterministic) ----------------\n",
    "def plot_fault_examples(series, series_len=SERIES_LEN, savepath=None):\n",
    "    start = 0 if len(series) > series_len + 1 else 0\n",
    "    x = series[start:start+series_len].copy()\n",
    "\n",
    "    # Create deterministic faulty versions using the deterministic injector params\n",
    "    faults = {\n",
    "        \"Drift Fault\": inject_drift(x, drift_rate=0.05, start_idx=20),\n",
    "        \"Bias Fault\": inject_bias_instances(x, bias_val=3.0, instances=(15,45,75), instance_len=8),\n",
    "        \"Stuck Fault\": inject_stuck(x, stuck_value_offset=5.0, start_idx=30, length=25),\n",
    "        \"Spike Fault\": inject_spike(x, amp=4.0, freq=6),\n",
    "        \"Erratic Fault\": inject_erratic(x, var_scale=0.6),\n",
    "        \"Data Loss Fault\": inject_data_loss(x, block_positions=(5,40,70), block_len=6)\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    t = np.arange(series_len)\n",
    "\n",
    "    for i, (title, y_fault) in enumerate(faults.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(t, x, label=\"Normal Data\", color=\"blue\", linewidth=1.0)\n",
    "        ax.plot(t, y_fault, label=title.replace(\" Fault\",\"\") + \" Fault\", color=\"red\", linewidth=1.0)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Sensor Output\")\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Confusion matrix plotting (Blues, light->dark) ----------------\n",
    "def plot_confusion_matrix(cm, classes, title, savepath=None):\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=cm.max())\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks); ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(classes)\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ax.set_ylabel('True label'); ax.set_xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Main experiment flow ----------------\n",
    "def run_all():\n",
    "    # 1) load series\n",
    "    series = load_singlehop_temperature(DATA_FOLDER)\n",
    "\n",
    "    # 2) build synthetic dataset\n",
    "    images, labels = build_synthetic_dataset(series, samples_per_class=SAMPLES_PER_CLASS, series_len=SERIES_LEN)\n",
    "    class_names = {0:'normal', 1:'drift', 2:'stuck', 3:'bias', 4:'spike', 5:'erratic', 6:'data_loss'}\n",
    "\n",
    "    # 3) Show and save GAF grid (3 samples each class)\n",
    "    print(\"\\nSaving GAF examples grid (jet colormap, fixed vmin/vmax)...\")\n",
    "    show_gaf_grid(images, labels, class_names, examples_per_class=3, savepath=os.path.join(OUTPUT_DIR, \"gaf_grid_3x7.png\"))\n",
    "\n",
    "    # 4) Save deterministic fault example line-plots\n",
    "    plot_fault_examples(series, series_len=SERIES_LEN, savepath=os.path.join(OUTPUT_DIR, \"fault_examples.png\"))\n",
    "\n",
    "    dataset = SyntheticGAFDataset(images, labels)\n",
    "    results_summary = []\n",
    "\n",
    "    for finetune_mode in FINETUNE_MODES:\n",
    "        print(f\"\\n=== Evaluating one-shot (K=1) with finetune_mode = {finetune_mode} ===\")\n",
    "        for backbone_name in BACKBONES:\n",
    "            print(f\"\\n-- Backbone: {backbone_name} --\")\n",
    "            backbone, feat_dim = get_backbone(backbone_name, pretrained=True)\n",
    "            backbone.eval()\n",
    "\n",
    "            all_true = []\n",
    "            all_pred = []\n",
    "            tsne_episode = None\n",
    "\n",
    "            for ep in tqdm(range(N_EPISODES), desc=f\"{backbone_name}-{finetune_mode} episodes\"):\n",
    "                sup_imgs, sup_lbls, qry_imgs, qry_lbls, classes_in_episode = dataset.sample_episode(n_way=7, k_shot=1, n_query=QUERY_M)\n",
    "                sup_t = torch.stack([transform(Image.fromarray(np.uint8((im - im.min())/(im.max()-im.min()+1e-9)*255)).convert(\"RGB\")) for im in sup_imgs]).to(DEVICE)\n",
    "                qry_t = torch.stack([transform(Image.fromarray(np.uint8((im - im.min())/(im.max()-im.min()+1e-9)*255)).convert(\"RGB\")) for im in qry_imgs]).to(DEVICE)\n",
    "                sup_lbls_t = torch.tensor(sup_lbls, dtype=torch.long)\n",
    "                qry_lbls_t = torch.tensor(qry_lbls, dtype=torch.long)\n",
    "\n",
    "                if finetune_mode == \"no_finetune\":\n",
    "                    with torch.no_grad():\n",
    "                        emb_sup = extract_embeddings(backbone, sup_t)\n",
    "                        emb_qry = extract_embeddings(backbone, qry_t)\n",
    "                elif finetune_mode == \"finetune_last\":\n",
    "                    head = finetune_head(backbone, feat_dim, sup_t, sup_lbls_t, n_way=7, epochs=EPOCHS, lr=1e-3)\n",
    "                    with torch.no_grad():\n",
    "                        emb_sup = extract_embeddings(backbone, sup_t)\n",
    "                        emb_qry = extract_embeddings(backbone, qry_t)\n",
    "                elif finetune_mode == \"finetune_whole\":\n",
    "                    backbone.train()\n",
    "                    classifier = nn.Linear(feat_dim, 7).to(DEVICE)\n",
    "                    opt = torch.optim.Adam(list(backbone.parameters()) + list(classifier.parameters()), lr=1e-4)\n",
    "                    lbls = sup_lbls_t.to(DEVICE)\n",
    "                    for _ in range(EPOCHS):\n",
    "                        opt.zero_grad()\n",
    "                        emb = extract_embeddings(backbone, sup_t)\n",
    "                        logits = classifier(emb)\n",
    "                        loss = F.cross_entropy(logits, lbls)\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "                    backbone.eval()\n",
    "                    with torch.no_grad():\n",
    "                        emb_sup = extract_embeddings(backbone, sup_t)\n",
    "                        emb_qry = extract_embeddings(backbone, qry_t)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown finetune mode\")\n",
    "\n",
    "                emb_sup = emb_sup.to(DEVICE)\n",
    "                emb_qry = emb_qry.to(DEVICE)\n",
    "                prototypes = compute_prototypes(emb_sup, sup_lbls_t.to(DEVICE), n_way=7)\n",
    "                preds = prototypical_predict(emb_qry, prototypes)\n",
    "\n",
    "                all_true.extend(qry_lbls.tolist())\n",
    "                all_pred.extend(preds.tolist())\n",
    "\n",
    "                if tsne_episode is None:\n",
    "                    with torch.no_grad():\n",
    "                        emb_support_np = emb_sup.cpu().numpy()\n",
    "                        emb_query_np = emb_qry.cpu().numpy()\n",
    "                    combined_emb = np.vstack([emb_support_np, emb_query_np])\n",
    "                    combined_lbls = np.concatenate([sup_lbls, qry_lbls])\n",
    "                    prototypes_np = prototypes.cpu().numpy()\n",
    "                    tsne_episode = (combined_emb, combined_lbls, prototypes_np, classes_in_episode)\n",
    "\n",
    "            # compute confusion matrix where labels are 0..6\n",
    "            all_true = np.array(all_true, dtype=int)\n",
    "            all_pred = np.array(all_pred, dtype=int)\n",
    "            cm = confusion_matrix(all_true, all_pred, labels=list(range(7)))\n",
    "            cm_path = os.path.join(OUTPUT_DIR, f\"cm_{backbone_name}_{finetune_mode}_1shot.png\")\n",
    "            plot_confusion_matrix(cm, [class_names[c] for c in range(7)], f\"Confusion Matrix - {backbone_name} - {finetune_mode} - 1-shot\", savepath=cm_path)\n",
    "\n",
    "            # metrics\n",
    "            overall_acc = np.trace(cm) / np.sum(cm)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(all_true, all_pred, labels=list(range(7)), zero_division=0)\n",
    "            print(f\"\\nBackbone={backbone_name} | finetune={finetune_mode} | 1-shot overall acc = {overall_acc:.4f}\")\n",
    "            for i, cname in enumerate([class_names[c] for c in range(7)]):\n",
    "                print(f\"  class {cname:8s}  prec={prec[i]:.3f}  rec={rec[i]:.3f}  f1={f1[i]:.3f}\")\n",
    "\n",
    "            # t-SNE visualization for representative episode\n",
    "            if tsne_episode is not None:\n",
    "                emb_combined, lbls_combined, prototypes_np, classes_in_episode = tsne_episode\n",
    "                try:\n",
    "                    tsne = TSNE(n_components=2, perplexity=min(30, max(5, emb_combined.shape[0]//5)), n_iter=TSNE_N_ITER, random_state=SEED)\n",
    "                    z = tsne.fit_transform(emb_combined)\n",
    "                except Exception as e:\n",
    "                    rng = np.random.RandomState(SEED)\n",
    "                    z = rng.normal(size=(emb_combined.shape[0], 2))\n",
    "                fig, ax = plt.subplots(figsize=(8,6))\n",
    "                unique_labels = np.unique(lbls_combined)\n",
    "                for c in unique_labels:\n",
    "                    idx = lbls_combined == c\n",
    "                    ax.scatter(z[idx,0], z[idx,1], s=30, label=f\"{class_names[classes_in_episode[c]]}\")\n",
    "                # mark support points\n",
    "                k_shot = 1\n",
    "                n_sup = k_shot * 7\n",
    "                ax.scatter(z[:n_sup,0], z[:n_sup,1], marker='X', s=120, edgecolors='k', linewidths=1.2)\n",
    "                ax.set_title(f\"t-SNE embeddings ({backbone_name}, {finetune_mode}) - representative 1-shot episode\")\n",
    "                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                tsne_path = os.path.join(OUTPUT_DIR, f\"tsne_{backbone_name}_{finetune_mode}_1shot.png\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(tsne_path, dpi=200, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "            results_summary.append({\n",
    "                \"backbone\": backbone_name,\n",
    "                \"finetune_mode\": finetune_mode,\n",
    "                \"k_shot\": 1,\n",
    "                \"overall_acc\": overall_acc,\n",
    "                \"cm_path\": cm_path\n",
    "            })\n",
    "\n",
    "            del backbone\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    df_res = pd.DataFrame(results_summary)\n",
    "    df_res.to_csv(os.path.join(OUTPUT_DIR, \"one_shot_summary_results_deterministic.csv\"), index=False)\n",
    "    print(\"\\nSaved summary to\", os.path.join(OUTPUT_DIR, \"one_shot_summary_results_deterministic.csv\"))\n",
    "    print(\"GAF grid saved to\", os.path.join(OUTPUT_DIR, \"gaf_grid_3x7.png\"))\n",
    "    print(\"Fault examples saved to\", os.path.join(OUTPUT_DIR, \"fault_examples.png\"))\n",
    "    print(\"Confusion matrices and t-SNE images saved under\", OUTPUT_DIR)\n",
    "    return df_res\n",
    "\n",
    "# Run the full analysis (this will take time; adjust EPOCHS and N_EPISODES for faster runs)\n",
    "if __name__ == \"__main__\":\n",
    "    class_names = {0:'normal', 1:'drift', 2:'stuck', 3:'bias', 4:'spike', 5:'erratic', 6:'data_loss'}\n",
    "    df = run_all()\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8286600,
     "sourceId": 13083582,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
